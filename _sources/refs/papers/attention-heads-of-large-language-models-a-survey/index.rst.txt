.. _attention-heads-of-large-language-models-a-survey:

Attention Heads of Large Language Models: A Survey
==================================================

:id: 2409.03752
:Authors: Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, Zhiyu Li
:Published: 2024-09-05
:arXiv: https://arxiv.org/abs/2409.03752
:PDF: https://arxiv.org/pdf/2409.03752
:DOI: N/A
:Journal Reference: N/A
:Primary Category: cs.CL
:Categories: cs.CL
:Comment: 29 pages, 11 figures, 4 tables, 5 equations

:github_url: _

abstract
--------
Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in
various tasks but remain as black-box systems. Consequently, the reasoning
bottlenecks of LLMs are mainly influenced by their internal architecture. As a
result, many researchers have begun exploring the potential internal mechanisms
of LLMs, with most studies focusing on attention heads. Our survey aims to shed
light on the internal reasoning processes of LLMs by concentrating on the
underlying mechanisms of attention heads. We first distill the human thought
process into a four-stage framework: Knowledge Recalling, In-Context
Identification, Latent Reasoning, and Expression Preparation. Using this
framework, we systematically review existing research to identify and
categorize the functions of specific attention heads. Furthermore, we summarize
the experimental methodologies used to discover these special heads, dividing
them into two categories: Modeling-Free methods and Modeling-Required methods.
Also, we outline relevant evaluation methods and benchmarks. Finally, we
discuss the limitations of current research and propose several potential
future directions.

.. include:: premise.rst

.. include:: outline.rst

.. include:: quotes.rst

.. include:: notes.rst

.. include:: summary.rst
