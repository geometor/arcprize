.. _combining-induction-and-transduction-for-abstract-reasoning:

Combining Induction and Transduction for Abstract Reasoning
===========================================================

:id: 2411.02272
:Authors: Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
:Published: 2024-11-04
:arXiv: https://arxiv.org/abs/2411.02272
:PDF: https://arxiv.org/pdf/2411.02272
:DOI: N/A
:Journal Reference: N/A
:Primary Category: cs.LG
:Categories: cs.LG, cs.AI, cs.CL
:Comment: N/A

:github_url: _

abstract
--------
When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.

.. include:: premise.rst

.. include:: outline.rst

.. include:: quotes.rst

.. include:: notes.rst