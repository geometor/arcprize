<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="2411.02272v3.Combining_Induction_and_Transduction_for_Abstract_Reasoning.pdf" name="source_pdf" />
<meta content="2024-11-25 20:39:30" name="summary_date" />
<meta property="og:title" content="Combining Induction and Transduction for Abstract Reasoning" />
<meta property="og:type" content="website" />
<meta property="og:url" content="refs/papers/combining-induction-and-transduction-for-abstract-reasoning/" />
<meta property="og:site_name" content="geometor • arcprize" />
<meta property="og:description" content="id, 2411.02272,, Authors, Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu,..." />
<meta name="description" content="id, 2411.02272,, Authors, Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu,..." />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Combining Induction and Transduction for Abstract Reasoning &mdash; geometor • arcprize</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=ea5fcf70" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />

  
    <link rel="canonical" href="https://geometor.github.io/arcprize/refs/papers/combining-induction-and-transduction-for-abstract-reasoning/" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=187304be"></script>
        <script src="../../../_static/doctools.js?v=9bcbadda"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../../../about/" />
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="Communicating Natural Programs to Humans and Machines" href="../communicating-natural-programs-to-humans-and-machines/" />
    <link rel="prev" title="Automated Design of Agentic Systems" href="../automated-design-of-agentic-systems/" />   
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../../log/atom.xml"
  title="geometor • arcprize"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../" class="icon icon-home">
            geometor • arcprize
              <img src="../../../_static/arcprize-logo-200.gif" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../mission/">mission</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage/">usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules/">modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logs/">logs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">references</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../">papers</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../a-divide-align-conquer-strategy-for-program-synthesis/">A Divide-Align-Conquer Strategy for Program Synthesis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../addressing-the-abstraction-and-reasoning-corpus-via-procedural-example-generation/">Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../alice-in-wonderland-simple-tasks-showing-complete-reasoning-breakdown-in-state-of-the-art-large-language-models/">Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../analog-bits-generating-discrete-data-using-diffusion-models-with-self-conditioning/"><span class="sectnum">1 </span>Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../arcle-the-abstraction-and-reasoning-corpus-learning-environment-for-reinforcement-learning/">ARCLE: The Abstraction and Reasoning Corpus Learning Environment for Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../attention-heads-of-large-language-models-a-survey/">Attention Heads of Large Language Models: A Survey</a></li>
<li class="toctree-l3"><a class="reference internal" href="../automated-design-of-agentic-systems/">Automated Design of Agentic Systems</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Combining Induction and Transduction for Abstract Reasoning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#abstract">abstract</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#premise">premise</a></li>
<li class="toctree-l5"><a class="reference internal" href="#outline">outline</a></li>
<li class="toctree-l5"><a class="reference internal" href="#quotes">quotes</a></li>
<li class="toctree-l5"><a class="reference internal" href="#notes">notes</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#summary">summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../communicating-natural-programs-to-humans-and-machines/">Communicating Natural Programs to Humans and Machines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../diffusion-for-world-modeling-visual-details-matter-in-atari/">Diffusion for World Modeling: Visual Details Matter in Atari</a></li>
<li class="toctree-l3"><a class="reference internal" href="../diffusion-on-syntax-trees-for-program-synthesis/">Diffusion On Syntax Trees For Program Synthesis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dreamcoder-growing-generalizable-interpretable-knowledge-with-wake-sleep-bayesian-program-learning/">DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../generative-agent-simulations-of-1000-people/">Generative Agent Simulations of 1,000 People</a></li>
<li class="toctree-l3"><a class="reference internal" href="../h-arc-a-robust-estimate-of-human-performance-on-the-abstraction-and-reasoning-corpus-benchmark/">H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="../learning-to-learn-at-test-time-rnns-with-expressive-hidden-states/">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a></li>
<li class="toctree-l3"><a class="reference internal" href="../on-the-measure-of-intelligence/">On the Measure of Intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/"><span class="sectnum">1 </span>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</a></li>
<li class="toctree-l3"><a class="reference internal" href="../planning-transformer-long-horizon-offline-reinforcement-learning-with-planning-tokens/">Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens</a></li>
<li class="toctree-l3"><a class="reference internal" href="../principled-instructions-are-all-you-need-for-questioning-llama-1-2-gpt-3-5-4/">Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../procedural-knowledge-in-pretraining-drives-reasoning-in-large-language-models/">Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reasoning-abilities-of-large-language-models-in-depth-analysis-on-the-abstraction-and-reasoning-corpus/">Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../relational-decomposition-for-program-synthesis/"><span class="sectnum">1 </span><span class="sectnum">1 </span>Relational decomposition for program synthesis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../searching-latent-program-spaces/">Searching Latent Program Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tackling-the-abstraction-and-reasoning-corpus-arc-with-object-centric-models-and-the-mdl-principle/">Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle</a></li>
<li class="toctree-l3"><a class="reference internal" href="../training-language-models-to-self-correct-via-reinforcement-learning/">Training Language Models to Self-Correct via Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tree-of-problems-improving-structured-problem-solving-with-compositionality/">Tree of Problems: Improving structured problem solving with compositionality</a></li>
<li class="toctree-l3"><a class="reference internal" href="../unraveling-the-arc-puzzle-mimicking-human-solutions-with-object-centric-decision-transformer/">Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../when-a-language-model-is-optimized-for-reasoning-does-it-still-show-embers-of-autoregression-an-analysis-of-openai-o1/">When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../repos/">repos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pages/">pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../youtube/">youtube</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../todos/">todos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">geometor • arcprize</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../">references</a></li>
          <li class="breadcrumb-item"><a href="../">papers</a></li>
      <li class="breadcrumb-item active">Combining Induction and Transduction for Abstract Reasoning</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/geometor/arcprize/blob/main/docsrc/refs/papers/combining-induction-and-transduction-for-abstract-reasoning/index.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="../automated-design-of-agentic-systems/" class="btn btn-neutral float-left" title="Automated Design of Agentic Systems" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../communicating-natural-programs-to-humans-and-machines/" class="btn btn-neutral float-right" title="Communicating Natural Programs to Humans and Machines" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
              <section id="combining-induction-and-transduction-for-abstract-reasoning">
<span id="id1"></span><h1>Combining Induction and Transduction for Abstract Reasoning<a class="headerlink" href="#combining-induction-and-transduction-for-abstract-reasoning" title="Link to this heading"></a></h1>
<dl class="field-list simple">
<dt class="field-odd">id<span class="colon">:</span></dt>
<dd class="field-odd"><p>2411.02272</p>
</dd>
<dt class="field-even">Authors<span class="colon">:</span></dt>
<dd class="field-even"><p>Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis</p>
</dd>
<dt class="field-odd">Published<span class="colon">:</span></dt>
<dd class="field-odd"><p>2024-11-04</p>
</dd>
<dt class="field-even">arXiv<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://arxiv.org/abs/2411.02272">https://arxiv.org/abs/2411.02272</a></p>
</dd>
<dt class="field-odd">PDF<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://arxiv.org/pdf/2411.02272">https://arxiv.org/pdf/2411.02272</a></p>
</dd>
<dt class="field-even">DOI<span class="colon">:</span></dt>
<dd class="field-even"><p>N/A</p>
</dd>
<dt class="field-odd">Journal Reference<span class="colon">:</span></dt>
<dd class="field-odd"><p>N/A</p>
</dd>
<dt class="field-even">Primary Category<span class="colon">:</span></dt>
<dd class="field-even"><p>cs.LG</p>
</dd>
<dt class="field-odd">Categories<span class="colon">:</span></dt>
<dd class="field-odd"><p>cs.LG, cs.AI, cs.CL</p>
</dd>
<dt class="field-even">Comment<span class="colon">:</span></dt>
<dd class="field-even"><p>N/A</p>
</dd>
<dt class="field-odd">github_url<span class="colon">:</span></dt>
<dd class="field-odd"><p>_</p>
</dd>
</dl>
<section id="abstract">
<h2>abstract<a class="headerlink" href="#abstract" title="Link to this heading"></a></h2>
<p>When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.</p>
<section id="premise">
<h3>premise<a class="headerlink" href="#premise" title="Link to this heading"></a></h3>
</section>
<section id="outline">
<h3>outline<a class="headerlink" href="#outline" title="Link to this heading"></a></h3>
</section>
<section id="quotes">
<h3>quotes<a class="headerlink" href="#quotes" title="Link to this heading"></a></h3>
</section>
<section id="notes">
<h3>notes<a class="headerlink" href="#notes" title="Link to this heading"></a></h3>
</section>
</section>
<section id="summary">
<h2>summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Brief Overview</p></li>
</ol>
<p>This paper explores the effectiveness of induction and transduction for few-shot learning in abstract reasoning tasks, using the Abstraction and Reasoning Corpus (ARC-AGI) as a benchmark.  The authors synthesize a large dataset of problems and train neural networks for both inductive (inferring latent functions) and transductive (directly predicting outputs) approaches. They find that these methods are strongly complementary, and ensembling them achieves near human-level performance.</p>
<ol class="arabic simple" start="2">
<li><p>Key Points</p></li>
</ol>
<ul class="simple">
<li><p>Induction and transduction methods, despite sharing the same architecture and training data, solve different types of ARC-AGI problems.</p></li>
<li><p>Induction excels at precise computation and composing multiple concepts, while transduction performs better on fuzzier perceptual concepts.</p></li>
<li><p>Ensembling induction and transduction significantly improves performance, approaching human-level accuracy on ARC-AGI.</p></li>
<li><p>The performance of induction models scales with test-time compute.</p></li>
<li><p>A novel data generation pipeline synthesizes a large dataset of ARC-AGI-style problems, starting from a smaller set of manually-written programs and using LLMs for augmentation.</p></li>
<li><p>The proposed method’s performance saturates quickly when increasing the number of manually labelled seed programs, but scales efficiently with compute.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Notable Quotes</p></li>
</ol>
<p>None explicitly provided in the excerpt.</p>
<ol class="arabic simple" start="4">
<li><p>Primary Themes</p></li>
</ol>
<ul class="simple">
<li><p><strong>Few-shot learning:</strong> The core focus is on learning from a limited number of examples.</p></li>
<li><p><strong>Inductive vs. transductive learning:</strong> The paper compares and contrasts these two distinct learning paradigms.</p></li>
<li><p><strong>Program synthesis:</strong>  The use of Python code to represent the latent functions connects the work to program synthesis research.</p></li>
<li><p><strong>Ensemble methods:</strong> Combining the strengths of different learning approaches is shown to be beneficial.</p></li>
<li><p><strong>Data generation:</strong> The creation of large, high-quality synthetic datasets is a significant contribution.</p></li>
</ul>
</section>
</section>

<div class="section">
   
</div>

           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../automated-design-of-agentic-systems/" class="btn btn-neutral float-left" title="Automated Design of Agentic Systems" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../communicating-natural-programs-to-humans-and-machines/" class="btn btn-neutral float-right" title="Communicating Natural Programs to Humans and Machines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, geometor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>