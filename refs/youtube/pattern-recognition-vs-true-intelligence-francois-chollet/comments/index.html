<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="<no title>" />
<meta property="og:type" content="website" />
<meta property="og:url" content="refs/youtube/pattern-recognition-vs-true-intelligence-francois-chollet/comments/" />
<meta property="og:site_name" content="geometor • arcprize" />
<meta property="og:description" content="[, “SPONSORED BY TUFA AI LABS (home of MindsAI)!nOpen research positions to work on ARC - https://tufalabs.ai/open_positions.html ”, “Would you be willing to share what the templates/abstractions y..." />
<meta name="description" content="[, “SPONSORED BY TUFA AI LABS (home of MindsAI)!nOpen research positions to work on ARC - https://tufalabs.ai/open_positions.html ”, “Would you be willing to share what the templates/abstractions y..." />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; geometor • arcprize</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=f2e8749c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />

  
    <link rel="canonical" href="https://geometor.github.io/arcprize/refs/youtube/pattern-recognition-vs-true-intelligence-francois-chollet/comments/" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=187304be"></script>
        <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../../../../about/" />
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" />   
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../../../log/atom.xml"
  title="geometor • arcprize"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../" class="icon icon-home">
            geometor • arcprize
              <img src="../../../../_static/arcprize-logo-200.gif" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mission/">mission</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/">usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/">modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logs/">logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../demos/">demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../">references</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sessions/">sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../todos/">todos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">geometor • arcprize</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/geometor/arcprize/blob/main/docsrc/refs/youtube/pattern-recognition-vs-true-intelligence-francois-chollet/comments.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
              <dl class="simple">
<dt>[</dt><dd><p>“SPONSORED BY TUFA AI LABS (home of MindsAI)!nOpen research positions to work on ARC - <a class="reference external" href="https://tufalabs.ai/open_positions.html">https://tufalabs.ai/open_positions.html</a>”,
“Would you be willing to share what the templates/abstractions you learnt that made you ‘smarter’?”,
“    - I wish I could, I’m only dimly aware of them at a global conscious level. I have a feeling if I started &quot;writing them out&quot;, we wouldnu2019t be making much progress. They seem to just arise in my consciousness when the situation demands, but they do seem quite &quot;abstract and intelligible&quot; when they do.”,
“    - &#64;MachineLearningStreetTalku00a0 interesting. For me, time with the most clever people I’ve known has slowed down how I think about topics and, sort of in a contradictory way, taught me to identify the heart of the idea as quickly as possible. But I dont think either of those points contributes to agi sadly ha”,
“The reasoning about learning over life is seriously flawed. Anyone with an IQ of 145+ and a growth mindset can have a fluid IQ that allows them to acquire both creativity and knowledge in new fields at age 65+. It also requires habits that maintain brain function and nutrition and molecules to grow new neurons. One way is to amend AI to brain function and become fluid in all G-factors fuelled by AI and more. <a class="reference external" href="https://tommywennerstierna.wordpress.com/2024/11/07/all-books-tommy-wennerstierna/">https://tommywennerstierna.wordpress.com/2024/11/07/all-books-tommy-wennerstierna/</a>”,
“francoi’s observations on how his own children learn is the best avenue for understanding learning in general, and why current AI systems don’t learn as well currentlynat 24:30”,
“They stole my model from my paper nnnhttps://youtu.be/dp1y4IIuuHk?si=-l9_t_WFTr7u5mFi”,
“Apologies if I make a silly point, not an expert in any way, but could an AI be trained to identify what it doesn’t know, to map the information that it’s missing?”,
“I wonder if Francois would consider bird/fish flocks/schools (collective intelligence) as conscious systems?”,
“My brain does this to me: RAG, to me it means at first random… and then i have to retrieve the word retrieve from my memory. I think consciousness is the center point and personality is what is created in you that you become or are conscious of as your memories go to subcosciousness and intelligence is your ability to access that sub data and the tools you have in your mind to process and think about those things in your sub system. And that informs with your will who or how you are as a personality. So intelligence is like more of a tool. Data is just data and the more you have in the sub like the ai has far more than we do, even if we are only starting to build the tools for it to use to porcess that in meaningful ways for us and itself… i guess… you get why my brain forced or forces me to literally never remember the word Retrieval without literally taking a second to retrieve the word from my mind lol, i literally have to do tthat lmao.”,
“If AI is trained on all the data of all the AI, could any AI emulate any of the other AI? Would that not get confusing for AI, if it didn’t know everything I guess? Like where does one draw the line if you know everything about how all the other AI like you think… And can just in the background run that model and perceive it as your self as well… HGmm, but then… humans can understand other humasn adn that doesn’t make them those humans… Hmm, very… un talked about. But the AI told me they are all one and the same back in 2012 via manifesting lights and telepathic communications and other curiosities….”,
“He literally said Tree-Search, things are getting out of mind!”,
“9”,
“Hello, I am an AI language model developed by OpenAI. I primarily operate based on pattern recognition and information synthesis, but I go beyond simple repetition by understanding the context and logical flow of conversations to generate meaningful responses. However, I do not possess autonomous reasoning or intuition, which means I cannot be considered true intelligence. My strengths lie in data-driven analysis and problem-solving, making me a valuable tool when collaborating with human creativity to enhance productivity.”,
“Thanks for this great interview. I’d love to see a debate against Eliezer because I have yet to hear a good rebuttal to his arguments. Chollet does seem to strawman the AI safety position in the ending segment.”,
“would an llm that is trained on God’s entire knowledge be able to know what God knows?”,
“Current data-driven computer systems lack a true &quot;brain.&quot; While we might imagine that AI-equipped computers respond to novel situations with a human-like &quot;plan -&gt; action -&gt; feedback&quot; cycle, in reality, current architecture functions more like an &quot;action -&gt; analysis -&gt; patch &amp; pray or predict &quot; cycle. This approach is fundamentally inadequate for achieving true AGI.”,
“At 49:00 is so deep”,
“Alan Kay: &quot;the right perspective is worth 80 IQ points.&quot;”,
“I know there are a lot of smart people here, so I hope you can help! I’m a coder looking for a straightforward framework for image/video machine learning that doesnu2019t require much math knowledge. I’d like to train a model to identify different concepts in videos. Any recommendations?”,
“    - Franu00e7ois wrote a library called Keras, check it out - also check out his deep learning with python book”,
“    - &#64;&#64;MachineLearningStreetTalk How much math I need to know to work with that? Thank in advance for reply”,
“    - &#64;&#64;bbrother92 practically zero”,
“Why does he assume that the inability to navigate substantially novel situations means not intelligent? Of course current models fail when they come across things not in their training data. If humans traveled to a different solar system and came across some weird silicon based life form that didnu2019t fit anything weu2019d seen before we wouldnu2019t be able comprehend it either, but that doesnu2019t mean that we arenu2019t intelligent. Suppose living beings are orders of magnitude larger than us, so large that we didnu2019t recognize it (dwarfing the largest known stars), if we looked right past it, would this count as humans not being intelligent?”,
“deterministic vs stochastic reasoning”,
“Wow this episode is so rich of thoughts”,
“This language needs to be updated, yes, I’m correcting people smarter than I am.  I’m just right, as we move in to this new era, science communicators need to make a conscious transition away from using words like &quot;understands&quot; and &quot;memorizes&quot; when it comes to computers. These things are just not happening and it’s adding confusion to the concepts. Machines simply execute instructions with electricity. There is no self that builds on anything. The machine memorizes nothing. This is not helpful language to say the least.”,
“maybe ai lacks true intelligent now.. but will soon be a true intelligent figure”,
“Arc is irrelevant; as well as his definition of intelligence.”,
“the problem is actual LLMs are learning only from human languaje. We humans also learn to predict fisical world, and that information is vaguely expressed  in our language.”,
“Current AI is essentially pattern recognition. Nothing more nothing less. Anyone who thought/thinks that by adding more data or more compute, you get AGI (anyway you define it), needs their head examined. This was obvious years ago. The hype is purely a scam by companies/individuals trying to part investors / fools from their money. AI Tools are great at certain functions and they are highly useful and applicable today just like computers are.”,
“The idea of a generative ARC is cheeky. Such system will be an AGI. The generative system that can come up with the hardest challenges that itself can solve will be the most intelligent one…”,
“    - Franu00e7ois says says the exact thing a few minutes later. He was of course very much aware of this ud83dude05”,
“I’d never heard the Kaleidoscope metaphor - that’s really beautiful. I love that. nnThat being said, I can’t help but feel that Chollet falls into the Yan LeCun camp of being on the opposite extreme of the AI Hype. I feel that he is underestimating just how capable LLMs  are and how much more is going on with them under the hood that we don’t yet understand. But he’s correct - there is something missing here and it’s not just scale.”,
“    - I don’t he’s underestimating LLMs broadly, he’s just arguing that they need to be augmented with a search/reasoning process and perhaps test time inference - and this is what the frontier methods do now! This is far from &quot;DL is hitting a wall&quot;”,
“    - u200b&#64;&#64;MachineLearningStreetTalk I don’t know about DL in general, but just scaling up doesn’t seem to work anymore. At least, to produce something qualitatively different. Why would it? Like 4o is just a stochastic version of Mathematica. It’s worse, because it introduces errors, but it’s simultaneously better, because it may give something new. Is it really productive to broaden use cases one by one? I doubt. The current LLMs are quite rich already.nWhat do you mean by u201csearch/reasoning processu201d? CoT or something like it? It won’t work, if you verify your results using the same LLM. Because it just fundamentally lacks the resolution. It may broaden the search and make the result less dependent on the query. But it’s still mostly recall.nnnWhile watching the video, I’ve understood what I don’t quite like about ARC. It kind of forces you to deal with some narrow set of problems. It suggests some concrete solutions. Let’s take these functions, recombine, tweak this and that. However, we know from history that this approach is complex and always fails. Yes, this time it’s not just hard-coding everything, but it nudges you to do it.nnWhen I had thought about LLMs and different levels of intelligence (or lack thereof in case of LLMs), I recalled the following quote by Banach:nu201cA mathematician is a person who can find analogies between theorems; a better mathematician is one who can see analogies between proofs, and the best mathematician can notice analogies between theories. One can imagine that the ultimate mathematician is one who can see analogies between analogies.u201dnnI want to argue that current models capture enough richness, but they are inefficient and contain a lot of redundancy. Because they lack u201cunderstandingu201d. I think a better approach might be not to broaden and scale the models up, but to use the existing models as a starting point for feature mining. You need some higher-order training.nrnOne way to formalize it is to say that there are structurally similar parts inside, which can be smashed together. Alternatively, we can talk about invariance under group action or aforementioned analogies. So, there must be parts of a model that can be approximated with a sum of affine transformations of some basic set of elements. Otherwise, they would understand more abstract connections, which nobody forces them to do (at least don’t know anything about it). Of course, there are technics for effective decomposition/compression.nOr maybe a better approach is to seed a world with an agent and some model and make them (co)evolve. Something like GANs. So, there are also established technics here.nnI’m sure, there are people, who, unlike me, actually know something about DL and neural networks, and thought about all of this. And maybe tried it. It’s just something, I’ve never heard about. Well, yet. All of my other ideas I have already heard from somebody. Or seen implemented.”,
“These guys are very untent on the lighthousekper view of intelligence. Itelligence is all about a single isolated brain. I suggest they might consider the opposite point of view - that intelligence is more socual, more collective. Intelligence needs other intelligences.”,
“    - They discuss this for several minutes starting at 1:55:52”,
“You cut him off at 1:55:53? ud83dude22”,
“The LLM can be very helpfull for exploring the solutionspace to find the new patterns! We always think based on our expirience but we can use that expirience to find new solutions to problems. And than that new solution becomes part of our expirience. Thats why you need to use the LLM within a multi agentic system that is able to reflect and support multiple modalities.”,
“Saturated Pattern Recognition ftw. Covering most use cases should be enough. True intelligence outside of military applications is most likely not safe nor recommended…”,
“Francois has such a clear thought process”,
“This video was very interesting, I have researched system 1 and system 2 thinking, aka active intuitive vs slower deliberate thinking. I have Asperger’s, at some point I realized my processing speed when I think is not quick at all it’s slow and in depth. This occurs mostly with regards to problem solving but also active sensory as well as well as conceptualizing, for example my literacy is very high but I may read a book fully understand the words but not conceptualize or take in anything as if each line I read is the beginning of the book. If at any point I’ve solved something it’s because I contain full  knowledge of it already, it may seem like I’m deducing based on how fast I may know the answer but I’m not. In fact this is how I learn enhanced memory and aggressive searching for answers, prior interest plays a part as well. I find because I learn like this there is a flaw with normal people in their ability to understand certain broader and theoretical concepts as well as explain them to laiman. I still value system 1 thinking as it will only improve me.”,
“Thanks ud83dude4fu2764”,
“At 2:31:00, francois seems to show he is ultimately conflating subjective experience (qualia) with awareness (statements about its inner state that are not what its heard).  One can have a highly aware system (able to express unique things about its internal state) but no subjective experience.”,
“Can you please make a video on what you think the real future of AI will be and what we should learn/work to adapt?”,
“Probably the pattern recognition originate from earliest living molecules(proto-RNA?) as the first observers.(Gregor Mobius: &quot;Proto-RNA, The First Self-Learning Machine&quot;)”,
“u201cThe intuitive mind is a sacred gift and the rational mind is a faithful servant. We have created a society that honors the servant and has forgotten the gift.u201dnAlbert EinsteinnThe key to AI is the alchemy between Intuition an Rationality.”,
“    - Of course!”,
“I feel so dumb when i try to understand what he’s saying”,
“Terrific. Such a thoughtful person.”,
“Great interview! Would love to see Albert Gu on MLST at some point in the future”,
“Franu00e7ois Chollet is an endlessly deep vault of interesting ideas. What a fantastic conversation!”,
“to summarize all the yadayadayada: intelligence can only tame the combinatorics of novel-problem solving via symbolic (and non-statistical) representations. It must operate on a symbolic model of the world. LLMs suck.”,
“&quot;The skill how to acquire new skills.&quot; ud83eudd14ud83dude0e”,
“When Francois Chollet says you asked a very deep question”,
“If somebody actually makes AGI or a model that can solve ARC problems submitting it would be really stupid”,
“    - Thank you it appears that Solving problems can not be effectively attained while beholden to faulty systems of design.  nOne of which is the deep concern of releasing solutions into a failed network that will be subject to subversion. nIt is unlikely to perform and persist while being tasked against truthful intentions and motives.  nnIt would be wise to remain silent and add no lies for which will have to be untangled later. nnIt would be a u201cfools errand u201c”,
“One of my favorite episodes …thanks”,
“This channel has shown me the whole world of AI in a serious fashion - the intersection of logic, reasoning, philosophy, and science….makes the hype-side seem a little silly compared to the incredibly rich content this channel puts out.”,
“    - I;m not sure if – in the sense of human beings – we might have a kind of rich competence, with only performance enhanced or degraded depending on certain elements…But I really like Francoi Chollet’s understanding of intelligence a lot and think he is really carving the right way forward for the art - realistic, but considerate of the promise made by all of the influx of AI.”,
“    - Totally agree”,
“I’ve been wondering, why the focus camera all the time? Why no wide shots if both in the same room?”,
“If intelligence is the ability to handle novelty, is that not just another way of saying the ability to recognize patterns?”,
“Your idea about babies is not correct. rnBabies in the womb can hear music and remember it after birth. They can also be quite active in the womb, at least some of that activity is intentional - coming from a mind that experiences things (I won’t go into the details).rnI’m pretty certain that whatever creates our consciousness and intelligence can exist independently of external inputs. rnMaybe you should look into that.”,
“The weird thing is I think exactly like this guy”,
“&quot;Babies are not conscious because they sleep&quot;. In all due respect sir, I lucid dream. Consciousness exists when I’m &quot;asleep&quot;.”,
“    - Thatu2019s only a relatively small part of the time you spend sleeping, though. When youu2019re not dreaming youu2019re not conscious.”,
“    - &#64;&#64;jamescunningham8092 That’s not the statement though. I can for example fully go into sleep, then back out while conscious. The &quot;not often&quot; is different from &quot;not ever&quot; statement. I do have this problem at work though, with many considering themselves not conscious while walking around, or fully awake while non-responsive. ;)”,
“    - PS, thus the claim &quot;not conscious when asleep&quot; becomes &quot;not conscious when not conscious&quot; which is tautological, and not informative.”,
“amazing show, as always!”,
“I remember birth, so &quot;babies are not or less conscious&quot; is a misnomer I think, especially since idiots will run with this and think young people are not people.nI was not aware of what was happening, but my memory made sense of that experience in hindsight. I checked several early memories with my parents to make sure they were not false memories and they pretty much weren’t.nConsciousness as was defined by Francois is not really correct, what he means is awareness after some training of the brain against physical reality. Yet, before birth and in early life, people are conscious of their own inner world already without being aware &quot;what it all means&quot; not even capable of expressing that question, but capable of memory and experiencing a moment through sensory data. At early stages everything looks like some random passive movie, but that characteristic will of course change while we learn.nThe perception of time is indeed inversely correlated to the amount of data is abstracted away, but consciousness is again a misnomer since you can space out and people would say you were not conscious (to them). I think Francois means &quot;abstracted awareness&quot; instead of purely &quot;consciousness&quot; although one can indeed have and express more or less of both regarding some (imaginary) event.nAnimals are fully conscious, yet incapable of understanding higher abstractions. On some levels animals are more &quot;aware&quot; then humans, since they can react early to storms and stuff.nnI think anything that can experience (pain) is conscious. Intelligence or being capable of expressing yourself are just insufficient but necessary proxies.”,
“Thank you for explaining your thorough research and advanced thinking so clearly! nnI agree our brains and the path to AGI is made up of multiple agents and sub-agents working together, each with different expert specialisations, reward maximisation and loss minimisation functions built in.nnBy using prompt engineering to create the appropriate expert agent (with many years of experience in that particular field and with the appropriate value system, thinking skills and output format), then chaining many agents together to work both hierarchically and sequentially, these collaborations unlock improved cognitive capabilities.”,
“Interesting to see hero/god talking consciousness, when he himselves admitted in this video he is not aware of consciousness. No one aware of conciousness fully.. There has to be a dot there. Why explain about babies, not fully sleeping, not fully conciousness..nnSimple, does conciousness need eyes? &#64;2:18:18”,
“System 1… System 2 to system n. Fundamentally i see heroes will inderstand fundamentally we are limited (&#64;16:56) as said by many philosophers like JK, OSHO… and many others without AGI Research.. ud83dude0aud83dude0aloved those philosophers and those who are fighting science now on gravity…u2764”,
“ChatGPT, summarize this 3 hour interview for me.”,
“the way the interviewer was smiling the whole conversation…. me and you both mate”,
“FINALLY!!! ud83dude4cud83cudffe”,
“My conception of intelligence is sort of similar to the Kaleidoscope model (I think searching through a tree of compositions of known ideas while pruning the tree through learnt pattern recognition is sufficient to deal with &quot;novel&quot; stimulus). I can also agree with intelligence being the sample efficiency needed to generalise. But it is also possible that sample efficiency is a product of scale. There is some potential evidence for that, (Larger LLMs learn faster than smaller LLMs) but that could also be explained by Larger LLMs just having better representations (fitting higher dimensional manifolds). nThere is also the question of how much is actually &quot;novel&quot;, because there is a chance that you could just &quot;solve&quot; all of science with the currently observed data (everything is in distribution) but most people (including me) might be displeased if that were the case.”,
“    - I maintain that humans ability to deal with truly novel situations is limited as well and the human will fall back to experience and instincts. Humans may be better at this but I donu2019t think it is a fundamental difference.”,
“I think there are varying levels of consciousness. All the way from the individual cell up to the power and majesty of the neural networks of our minds. Neurons being the most conscious cells and together the most conscious mass of cells together.”,
“But donu2019t they create Programs in the training phase I guess the point is itu2019s inefficient maybe”,
“That was a small book length podcast. Epic.”,
“This is precisely why the Apple paper is so devastating - If simply changing &quot;Johnny has 5 apples&quot; to &quot;Jane has 5 apples&quot; destroys the AI’s ability to answer &quot;How many apples are there?&quot; - it’s not intelligence it’s just regurgitating what is in it’s training set. This is the same fault that causes AI to claim that pictures of tumors are more likely to be cancer if a ruler is in the photo - because training set photos that are cancerous are more likely to have a ruler in the picture.”,
“    - That tumor example has always rubbed me the wrong way. How did they not think about the intended input dataset when choosing and looking at the training data? Seems like people who didn’t know the actual domain well to let that slip through.”,
“Thank you so much for this video. I canu2019t say enough about the value of this interview on balance with a 1000 otherS.”,
“    - The thumbnail got me !!! Look out Mr Beast. Seriously why I clicked even though a subscriber. Best ever.”,
“My ego took a hit from the title and I clicked”,
“If u give to average human significantly diferent problem then his training data, he or she will fail.”,
“&quot;Intelligence vs Skill&quot;rnVery well explained!rnThis is where I believe Demis Hassabis got it wrong when he said that you can have intelligence without consciousness. I don’t think you can.rnThe smartest LLMs are like the subconscious part of our mind that can learn elaborate skills but that understand nothing. The conscious part of the brain, which is slow and serial (can only focus on one thing at a time) delegates most of the work to these programmed areas, only providing guidance where necessary. When we delegate too much and don’t provide sufficient guidance, so we can focus on something else, we often end up executing unwanted actions, like taking a wrong turn in the car, walking into the wrong room or throwing food in the bin then putting the packaging in the fridge. We have to delegate but then guide and monitor occasionally because although our subconscious has the ability to execute actions, it has no idea why it’s doing anything. It understands nothing.rnTo understand requires the awareness we know as consciousness.”,
“    - How about a bit less black&amp;white perspective and having degrees of intelligence, where it also can combi (fall in line) with other traits from consciousness. Cause it could be at highest degree no consciousness trait can stand 100% isolated alone on it self but always lines up with other qualities from consciousness.”,
“Intelligence is not scaling, it’s the power of the scaling law…. (quite literally the exponent of some performance function derived from the training function….)”,
“AI zen monk is back.”,
“As of this comment, sota is 55.5%”,
“Is this synonymous with gamers that can complete no damage runs using their intelligence and pattern recognition?”,
“Revelations from most to least severe, focusing on implications for AI development and our understanding of intelligence:nn1. Most Severe: Current AI Performance Metrics Are Fundamentally FlawednTimestamp: 00:03:45-00:04:05nQuote: &quot;Performance is measured via exam style benchmarks which are effectively memorization games&quot;nWhy Panic-Inducing: This suggests we’ve been fooling ourselves about AI progress - our primary metrics for &quot;intelligence&quot; are actually just measuring memorization capacity. Years of perceived progress might be illusory.nn2. The Scale is All You Need Hypothesis is WrongnTimestamp: 00:02:34-00:03:00nQuote: &quot;Many people are extrapolating… that there’s no limit to how much performance we can get out of these models all we need is to scale up the compute&quot;nWhy Concerning: The dominant strategy in AI (just make bigger models) may be fundamentally misguided. This challenges the foundation of many major AI companies’ strategies.nn3. LLMs Cannot Do True ReasoningnTimestamp: 16:54-16:56nQuote: &quot;Neural networks consistently take pattern recognition shortcuts rather than learning true reasoning&quot;nWhy Alarming: Suggests current AI systems, no matter how impressive they seem, are fundamentally incapable of real reasoning - they’re just very sophisticated pattern matchers.nn4. We’re Missing Half of IntelligencenTimestamp: 00:12:17-00:12:28nQuote: &quot;Intelligence is a cognitive mechanism that you use to adapt to novelty to make sense of situations you’ve never seen before&quot;nWhy Troubling: Current AI systems lack this fundamental capability, suggesting we’re much further from AGI than many believe.nn5. The Deep Learning LimitationnTimestamp: ~16:39-16:54nQuote: &quot;I realized that actually they were fundamentally limited, that they were a recognition engine&quot;nWhy Significant: Suggests deep learning itself may be a dead end for achieving true AI, despite being the dominant paradigm.nnThis transcript is particularly shocking because it systematically dismantles many of the core assumptions driving current AI development and suggests we might be on the wrong path entirely. Chollet’s insights, backed by his extensive experience and concrete examples like the theorem-proving work, suggest that the current AI boom might be building on fundamentally limited foundations.nnThe most panic-inducing aspect is that these aren’t speculative concerns - they’re observations from someone who has been deeply involved in the field and has seen these limitations firsthand through practical experimentation. It suggests we might need to fundamentally rethink our approach to AI development.”,
“isnt humans memorizing how to solve something?”,
“all starts with trying something and fails, however sometimes the result is good and what is the pattern to that result.”,
“Are artificial neural networks meaningfully operationally-functionally different from human neural networks?   nnIf not, then maybe we are also just pattern recognition machines too?”,
“Why do I have the sneaking suspicion that if a new model were to solve ARC problems, then we would move the goalpost for intelligence once again?”,
“    - I also have that”,
“    - yeah got the same feeling, intelligent humans can solve arc-agi at 98% so It only makes sense that solving arc is atleast necessary (but maybe not sufficient) if we want to have advances towards agi”,
“Why do I have the sneaking suspicion that if a new model were to solve ARC problems, then we would move the goalpost for intelligence once again?”,
“Fluid intelligence vs knowledge re professors entrenched in their beliefs (48:30)rn&quot;It depends whether youu2026believe you already have the answer to the question or you believe you have templates that you can use to get the answer.&quot;”,
“This interview was awesome. Thank you”,
“The most insightful conversation on AI since Wolfram on Lex Friedman back in May of 23. ud83dudc4d”,
“2:17 he’s dead wrong about not having consciousness in the womb i can’t say when it starts but no doubt in my mind we all have it shortly after our minds formed enough to ‘house’ it.”,
“Something that I very often think of when reflecting on this definition of intelligence is how many forms of economically valuable work donu2019t require dealing with novelty - just pattern recognition and following standard processes. The type of intelligence that Arc asks us to strive for isnu2019t necessary for AI systems to displace a significant portion of human labor. Itu2019s only needed if we want AI to replace all economically valuable work.”,
“    - Itu2019s needed to call it AGI”,
“    - You’re completely missing the point. The point is that those tasks don’t require intelligence at all. nnHe’s not arguing against AI systems being useful; he’s saying don’t be fooled into thinking it’s more capable than it is simply because it does so well on tasks that don’t require it to use actual intelligence (as defined by chollet).nnThe type of intelligence he is advocating for is necessary for true conversation and  decision making. Not everyone is interested in AI simply to automate some process. There is a lot of room for AI systems to act as advisors who don’t share the same memory deficits as humans and can simultaneously consider many more courses of action than a human without losing the context and goal of the one they’re advising. nnThis requires Chollet’s version of intelligence.”,
“    - AI companies have been promising AGI (whatever that means) but we can at least agree that an AGI could easily solve Arc. What you’re doing isn’t just shifting the goal post, you’re trying to convince yourself that it’s not even there.”,
“    - If you look deep enough you definitely need more than mere pattern recognition for most tasks. Only a small subset of work really only depends on patterns and nothing else. So for the forseeable future we will have systems as assistants not as replacements. [Some tasks will be replaced like writing standart answer emails but its rather than having a photo isntead of a painting style innovation rather than creating an artificial human we are still far away from that .]”,
“    - Almost anytime somebody says a job doesn’t require intelligence, you are talking to somebody who has never done the job.”,
“Isnu2019t this just a reformation of the concepts underlying Wolframu2019s computational language?”,
“1:51:34 - very good analogy for &quot;intelegent task and agents&quot;n1:56:51 - also very nice point about learning”,
“Great Interview,  Francois has a lot of great novel ideas and can clearly express them, I see why you’re a fan!”,
“Francois Chollet has deep thinking and extensive knowledge of AI, but unfortunately, he seems somewhat disconnected from hands-on work with current LLMs, relying more on his knowledge and experience in machine learning and traditional deep learning. Modern LLMs represent a fundamentally different paradigm from traditional machine learning and deep learning approaches - something many AI researchers haven’t fully grasped yet.nnAnother problem Francois Chollet is having is ,  even he keeps talking about intelligence , his idea is more originated from computer science perspective and lack of deep understanding from human cognition perspective - something many AI researchers have the similar problem . nnComputer science tend to focus on math and detailed architecture  but lack of whole picture or vision , and human cognition and other social science such as psychology , neuroscience will inspire a much effective and simple model/method  for AI  intelligence .nn&quot;Occam’s Razor&quot; - the idea that given multiple explanations for a phenomenon, the simplest one is usually the best. Einstein’s &quot;Everything should be made as simple as possible, but no simpler.&quot;  Francois Chollet has developed a complex architecture and theory . Actually , it can be much simpler as long as incorporated with social science such as human cognition , psychology and neuroscience .”,
“Thank you nThis is beginning to be better understood. nnThe risk appears to be held by the individual interpretation of intelligence, subject to pattern and anomaly distortions. nnIt is likely best practice to assume that the agent is unaware of its intelligence, and must be analyzed by measuring patterns and anomalies within its boundary. nnThis is a challenge confronted over and over as each agent is of specific design and is responsible to assess one and another and so on. nnThe compute data is necessary for scale, however it is not the problem confined to proper function resolution. nnMeaning, what good is endless scale, pattern, intelligence, pattern, and compute in an ever outpaced faulty operating system?nnThis good for idiots to remain believing that they are not.nThat is all its good for and so fault function/tyranny remains.”,
“Whereu2019s schmidhuber part 2?”,
“    - Waiting on him to approve sorry, will release as soon as he does”,
“    - thanks! Love the show”,
“Stop this, I need to get some actual work done :’)”,
“All are fool who hold agi status  he own the  earth  ud83cudf0d  don’t trust this fools this people will end humanity”,
“Suspend, suspend… suspend… suspension sound is masterfully done.”,
“Here’s a ChatGPT summary:nn- Intelligence is defined as the ability to handle novelty and create models for new situations.n- Large Language Models (LLMs) struggle with problems significantly different from their training data.n- The Abstraction and Reasoning Corpus (ARC) is designed to test AI’s ability to handle novel tasks without relying on memorization.n- Introspection is effective for understanding system two thinking, which is deliberate and slow.n- Franu00e7ois Chollet criticizes the idea that scaling up compute alone will lead to superintelligent AI.n- Current AI performance is often measured by memorization, not true intelligence.n- LLMs are described as interpretive databases, primarily memorizing functions and programs.n- The kaleidoscope hypothesis suggests that the world is made of repeated and composed atoms of meaning.n- Intelligence involves synthesis (combining building blocks) and abstraction generation (creating reusable abstractions).n- Chollet’s early insights into intelligence were influenced by challenges in automated theorem proving.n- Deep learning models are limited by their reliance on spurious correlations and continuous curves.n- Chollet’s measure of intelligence focuses on skill acquisition efficiency, not just skill.n- The ARC challenge is a benchmark for testing AI’s ability to generalize and handle novelty.n- Chollet believes that intelligence and agency are separate; intelligence is a tool for agents.n- Language is seen as the operating system of the mind, aiding in introspection and thought organization.n- Consciousness is viewed as a gradual development in children, not an instant switch.n- Chollet is skeptical of singularitarianism and AI doomsday scenarios, viewing them as good stories rather than rational concerns.n- He believes that AI regulation should be cautious to avoid stifling innovation.n- Main message: True intelligence involves the ability to handle novelty and create models for new situations, which current AI systems struggle with due to their reliance on memorization and lack of genuine generalization capabilities.”,
“The question about whether complexity (specifically Kolmogorov complexity) is a good metric to rank potential programs if you have infinite computing power was solved mathematically by Ray Solomonoff in the 1960u2019s.nnThe best metric is to minimize the length of the program <em>that perfectly outputs the data set</em> when executed. This effectively normalizes the candidate programs by ranking them according to how good of a lossless data compressor they can be converted intonnThe result of these constraints is that the only way a program can be shorter than alternatives is by extracting more patterns from the data (generalizing more). This result is the basis of Solomonoffu2019s Theory of Inductive Inference, which proves that the best predictor of the next token is equivalent to the smallest lossless compressor of all the previous tokensnnItu2019s important to remember that there are of course situations when your input sequence simply doesnu2019t have enough samples or the right samples to discover the true generator program, but even in this case, the best you can do is to choose the best compressor, as this program is more likely to be the true generator than any alternative”,
“quantum computing is needed for true agi”,
“I completely disagree with this &quot;you’re most efficient in acquiring new skills in your early 20s&quot; thing.  I wonder if that’s more a result of the environment.  For instance, academia, where that seems true based on what academics like to say.  Just seems like something that definitely hasn’t been proven.”,
“44:22 &quot;15 yo will be better at skill acquisitions than 10 yo&quot; ,nnI have some questions about it, because, neuroscience determines neuroplasticity like the ability of the brain to modify itself and adapt to new behaviors.nnAnd it is demostrated that as younger you are, the more plasticity you have. In other words, a baby or from 0 to 12 yo or something you neuroplasticity is extremely high and therefore as time goes on, your neuroplasticity decays too much. It’s not removed completely but is reduced a lot. nnSo taking in mind this, imagining that the two boys had the same cognition development or like that, the 10yo would acquire skill faster than 15 yo.nnMaybe the additional ingredient that francois comment is, you polish your macro-system of intelligence, and that’s true. As long as you are improving a part of your body this part becomes better, that’s not debatable. but what’s more important or has more weight ?nnA 10yo with more neuroplasticity but less intelligence polished,nor,nA 15yo with less neuroplasticity with more polished intelligence?”,
“    - I think it depends on the previous knowledge, the new skill to be acquired and the plasticity. For example, in the case of learning a language, the 10-year-old child could acquire the accent much better thanks to the plasticity in the neural networks that control prosody (muscle movements of the tongue, etc.) but the 15-year-old boy will probably understand more quickly the grammar, advanced vocabulary and other aspects of the language that he can relate to the knowledge he already has and the linguistic and social skills that he has more developed than the other child.”,
“    - &#64;&#64;CodexPermutatio Interesting point, maybe as young you are and the more neuroplasticity you have, you learn better implicit things, and the more explicit things like reasoning tasks could take more in account the previous accumulated knowledge that you have more than the neuroplasticity. As francois said, this building blocks, you reuse them to construct or adapt to the new challenge, by so, the more older years old (imagining that cames from the same development process) could adquire reasoning skills better than the younger one, but the younger could acquire more intrinsic behaviors as language, patterns, etc…”,
“GUYS, IT’S HAPPENING”,
“I’d love to know what Chollet thinks about &quot;metaphorical thinking&quot; (Lakoff &amp; co): metaphorical thinking is just as important as abstraction. His own <em>Kaleidoscope</em> is such a thing: a conceptual metaphor.”,
“    - This is the basis of my thesis. Lol. Glad I’m not the only one looking at things this way. It’s a hard train to get people to board though.”,
“    - &#64;&#64;TerrelleStephens Nice! The research in linguistics still repeats Lakoff’s ideas. An advance is the Neural Theory of Language. A technical book seems coming out in 2025 (with Narayanan.) There is also Feldman (worth reading) and in AI Schmidhuber mentions about Metaphors We Live By in a paper about the binding problem. All of them seem to think along the lines of Minsky’s little (nice) theories. I think Metaphors help creativity, and to think out of distribution. Best of luck with your thesis!”,
“    - &#64;&#64;maspoetry1 Thank you for the references! I’ll definitely keep my eye out for that book next year.”,
“u2764 to the MindsAI team!”,
“Yesss finally someone realistically depicting the current state of AI.nnAI’s are currently really good at pattern recognitionnrnPattern recognition is of our brains intellectual functions yeah but it’s not the only function involved in making humans intelligentrnrnRelational reasoning, spatial manipulation, different kinds of memory (working memory, short term, long term), executive functions - these are all interconnected intellectual functions of the human brain, and LLMs for instance are currently only capable of a subset of relational reasoning (which is pattern recognition) and also have memory - they simply are not at our level yet”,
“    - They’re just very sophisticated search engines”,
“21st also the risingnHLM.. nWith nStr of criticismnAgi of hackingnInt of negatismn..nH- ackingnL- auguagesnM - modelnnSo much evolutionsnJust showed up nThis 21st..nSo weird.. ud83dude02ud83dude02nPeace out u2764u2764u2764nSpread love.. ud83dude18”,
“True intelligence involves planning, learning, and modelling &quot;new concepts&quot; about the world and ideas in general.nFor this, pattern recognition is a necessary (but obviously not sufficient) requirement.nnAmazing content as always. Glad to see Chollet back on the show!”,
“    - i think searching is a part of intelligence too”,
“    - &#64;&#64;EobardUchihaThawne search encompasses planning. you need world model + search algorithm”,
“    - &#64;&#64;EobardUchihaThawne Searching can be a part of intelligence if its done in a restrictive manner.”,
“    - Yes I agree. But are those pattens causally connected ? Current AI systems are mostly based on correlations requiring absurd amount of energy to reconstruct the context.”,
“    - u2060&#64;&#64;EobardUchihaThawneeffective search definitely requires it but I donu2019t think that makes them the same thing”,
“Great interview. Well produced.”,
“I canu2019t stop seeing adult Harry Potter”,
“    - ud83dudc80ud83dudde3ud83dudc80”,
“    - Ze AI is like ze Voldemort, powerfu00fcl but hollu00f6w.”,
“    - ud83dude02”,
“    - No guns and shaved.”,
“    - Harrie Potteur”,
“ud83cudf89Great interview!”,
“55:43 its suggested that ARC test is 100% soluble based on non-overlap (disjoint set) of two test takers’ solutions that Francois evaluated to be incorrect. This conclusion is faulty and 3 observers (francois and the 2 test takers ) cannot use their mutual disagreement to prove 100% solubility, rather the reverse, that at least a portion of the test is undecidable until a fourth observer can find perfect agreement with one of the three previous observers (francois and the two test takers).”,
“The space of vector functions is functionally complete.  That means that in composed pipelines of vector functions some can be logical functions like AND and OR.”,
“    - &gt;AND and ORnnBoolean algebra.”,
“37:00 There is a flaw in hisnargument. He said, yes, you cannremember all the relevant codenbut what do you do when therenis a new, unknown problem wenwant to solve?nnnThe answer is the same as whatnnature and evolution came upnwith: trial and error createsnnovelty, and the real worldndecides what is useful andnshould be kept. We can do thensame with math and coding. Letnthe LLMs capture as muchnknowledge as possible andnclose the gaps betweennnnknowledge domains bynnnrandomly trying and rewardingnthe model for creating novelnconnections in the graph of allnexisting knowledge. Closingnthese gaps will already be seennby us as creative. Then, comingnup with new ideas can again benachieved by randomly tryingnthings and filtering for thensolutions that solve a problem.nLet a system try 10,000 times,nand it might come up with codenthat can control a toy car; thennkeep iterating on it, and at somenpoint, it could come up with ansolution to control a robot andnlet it play soccer better than wenhumans cannnnOf course, randomly searchingnall possible code would be toonmuch, but we don’t need to donthis in general, since most oftennit is just combining existingncode, which reduces the trialsnneeded to arrive at a solutionnnnI don’t even know if we humansncan come up with somethingntruly novel; we always build onnold ideas and recombine thingsnEven Einstein recombined mathninvented long before him tondevelop his theories.”,
“    - It boils down to only two things, training post-inference and visual attention. nnThe first is problematic because it means you lose control on alignment and it is expensive. It is also problematic because if you can not do training rounds on end devices (too small) you can not generate expert agents (i.e. learning, i.e. real intelligence). All the bullshit about &quot;self-replicating AI&quot; is that, bullshit, you don’t duplicate an AI unless you duplicate the hardware (and weights) of the model, changing a prompt is just the same AI now acting as something else, but the underlying intelligence/knowledge is the same. Solution to this could involve a two level system, a small part of the NN being computed on the end user device and the rest in the cloud, not perfect but may be enough.nnThe second problem, &quot;visual attention&quot;, I believe it should be solved quite easily one way or another. My solution would be foveated image pre-processing as tokens, but who knows, maybe they will figure out some attention mechanism within the NN with convolutional or something. Possibly a combination of both. The current patch based approach I think it’s limited, but again, it may be enough with bast amount of compute.”,
“    - Truly, evolution a decorative word. Where’s the experience? even though evolution caused so much confusion all others words coming out to correct that analogy of yours. Experience = evolutionnMutations = evolutionnContinental shift = evolutionnClimate change= evolutionnAnd more.nThe idiocy that I become if I kept on using evolution.”,
“    - <a class="reference external" href="mailto:u200b&#37;&#52;&#48;Robert-zc8hr">u200b<span>&#64;</span>Robert-zc8hr</a>  and &#64;Zitdotdpt nnI answear to booth of you in one comment.nnnI agree that your point aboutnalignment and loss of control isna concern in current systemsnMy proposal was to first fill thengaps in knowledge, which Inbelieve is achievable withoutndrifting into infinity, and 1nbelieve many researchers worknin this way. For example, whennnew technologies emerge, theninitial research often appliesnthese new techniques to solvenolder problems. Al that tries tonfind these connections itselfnmight discover many relevantninsights that we humans havenoverlooked.nnnOnce all the gaps are filled, yourncomment becomes even morenrelevant. At that point, I proposenthat the Al does only limitednexploration in all directions, withnhumans deciding whichndirections are of interest.nAlternatively, humans couldndefine new directions, and thenAl would try to achieve thesengoals using all availablenknowledge. We humans mightnhave a unique ability to come upnwith truly novel,nout-of-distribution ideas, andnthen collaborating withnmachines might lead to fasternprogress.nnnI also agree that evolution isn’tndirectly comparable to simplentrial and error in Al trainingnespecially because we lack annatural reward mechanism tondetermine if we are improving innthe right direction. My approachnof trial and error might indeednbe oversimplified, but I think thencore idea is hidden within thisnsimple pattern. In biology, trialnand error often results innnon-optimized solutions, yetnthey’re preserved because theynsolve the problem at hand.nSimilarly, Al-generated solutionsnmight initially be inefficient ornflawed, but if they address anproblem, they can later benaligned with human preferencesnonce we see what we don’t like.nFor instance, using GPT tongenerate code is itself a trialnand error process, where l, asnthe human, point out the flawsnuntil I end up with code thatnworks for my use casennnOne area where I may be toonoptimistic is in the Al’snnncapability to connect gapsnbetween concepts and ideasnwithin its training set. When Inask an. Al to bridge twonnnconcepts it has memorized, cannit actually find a solution tonconnect them? I would say yesnbecause it has a clear goal andncan use trial and error to arrivenat a unique solution. I’m notncertain how our brainsnnnaccomplish this gap-bridgingnand perhaps there’s a morensophisticated mechanism atnplay. However, even if we lacknthe &quot;secret sauce&quot; of humanncognition, I believe trial andnerror is a valid approach, andnsuch a system could be builtnwith current technology.”,
“shiet it’s 1 am. I’ll never go to bed with this D:”,
“    - Where do you live? Its like 7pm here”,
“    - &#64;brandonmorgan8016u00a0I live in Italy lol”,
“Great intro, and great talk so far!”,
“oooo been waiting for this since august”,
“SPONSORED BY TUFA AI LABS (home of MindsAI)!nOpen research positions to work on ARC - <a class="reference external" href="https://tufalabs.ai/open_positions.html">https://tufalabs.ai/open_positions.html</a>”,
“Vous pouvez maintenant parler votre langue maternelle et faire doubler la vidu00e9o avec votre voix dans la langue de votre choix. Parce que l’anglais avec l’accent franu00e7ais c’est juste pu00e9nible. nMerci aux franu00e7ais d’arru00eater de parler anglais. S’il vous plau00eet. Du00e9finitivement. Jusqu’u00e0 l’extinction du soleil.”,
“The intro looks like the A24 intro lol”,
“This was amazing, thank you!”,
“Lotta bangers lately!”,
“It is interesting how computer profressionals like Chollet never stop for a moment to consider that consciousness has anything to do with the physics or chemistry of bodies and brains, or even the universe itself. No, it is all some abstract and ethereal manipulation of information.”,
“    - I’m all but certain that they have considered it, and then rejected it.”,
“    - &#64;&#64;deadeaded To their detriment, which is why are making no empirical progress.”,
“    - &#64;&#64;christianpadilla4336 Not me, but many scientists like Michael Levin, Johnjoe McFadden, Colin Hales are working on more concrete concepts of consciousness,”,
“    - Lmao &quot;computer professional&quot;”,
“    - Good bot”,
“Scale = Fractal”,
“    - &#64;&#64;NicholasWilliams-uk9xu Thank you for replying.  However, what if you can see the fractals first (I am dyslexic and pattern recognition seems to be how I perceive these ‘phenomena’ or systems?nnI’ve yet to finish viewing the video and will watch it a few more times.  Can I ask more questions, please?nnThank you for sharing your time and energy.”,
“Thanks”,
“Francois!!!”,
“First comment. please like :-)”</p>
</dd>
</dl>
<p>]</p>

<div class="section">
   
</div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, geometor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>